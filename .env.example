# Generate a Fernet key with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW_FERNET_KEY=your-fernet-key-here
# Choose your data warehouse type: snowflake, bigquery, postgres, redshift, etc.
DBT_TARGET_TYPE=snowflake

# Connection string format depends on your warehouse type:
#
# For Snowflake:
# AIRFLOW_CONN_DWH_DEFAULT=snowflake://username:password@account/database/schema?warehouse=warehouse_name&role=role_name
#
# For BigQuery:
# AIRFLOW_CONN_DWH_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__project=your-project&extra__google_cloud_platform__keyfile_dict={"type":"service_account",...}
#
# For Postgres:
# AIRFLOW_CONN_DWH_DEFAULT=postgres://username:password@hostname:5432/database
#
# For Redshift:
# AIRFLOW_CONN_DWH_DEFAULT=redshift://username:password@hostname:5439/database

AIRFLOW_CONN_DWH_DEFAULT=your-connection-string-here

# ==========================================
# Example Snowflake Configuration
# ==========================================
# Uncomment and fill in if using Snowflake:

# AIRFLOW_CONN_DWH_DEFAULT=snowflake://MYUSER:MYPASS@xy12345.us-east-1/CYBERSECURITY_DB/PUBLIC?warehouse=COMPUTE_WH&role=ACCOUNTADMIN

# ==========================================
# Example BigQuery Configuration
# ==========================================
# Uncomment and fill in if using BigQuery:
# 1. Create a service account in GCP
# 2. Download the JSON key file
# 3. Mount it in docker-compose.yml volumes section
# 4. Set the connection string:

# AIRFLOW_CONN_DWH_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__project=my-project&extra__google_cloud_platform__key_path=/opt/airflow/keys/gcp-key.json
